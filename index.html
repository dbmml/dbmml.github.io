<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Distance-based methods in Machine Learning</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<span class="logo"><img src="images/logo_dbmml.svg" alt="Workshop logo, depicts a plot of two probability densities." /></span>
						<h1>Distance-based methods <br />in Machine Learning</h1>
						<p>Workshop <a href="https://www.ucl.ac.uk">@UCL</a>.
						27-28 June, 2023.</p>
						<ul class="actions special">
										<li><a href="https://www.eventbrite.com/e/workshop-on-distance-based-methods-in-machine-learning-tickets-623172193807" class="button primary">Registration</a></li>
					  </ul>
						<nav id='navlink'><h3>ðŸ”Š New & improved venue: <a href="#location">Bentham House</a></h3></nav>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#about" class="active">About</a></li>
							<li><a href="#schedule">Programme</a></li>
							<li><a href="#speakers">Speakers</a></li>
							<li><a href="#organisers">Organisers</a></li>
							<li><a href="#location">Location</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Introduction -->
							<section id="about" class="main">
								<p>Distance-based methods represent a varied and extensively used set of techniques for performing statistical learning by minimising the distance or discrepancy between probability distributions. One key advantage of distance-based techniques is that the resulting model's properties are dependent on the underlying distance selected. Crafting distances that encode desirable properties, such as stability and robustness, is a promising area of research.</p>

								<p>The workshop on will cover a broad range of statistical and machine learning methods, including but not limited to
												<ul style="margin-top:-30px;margin-bottom:7px;">
													<li>parameter estimation,</li>
													<li>generalised Bayes,</li>
													<li>hypothesis testing,</li>
													<li>optimal transport,</li>
												</ul>
								which are based on statistical distances such as the Maximum Mean Discrepancy (MMD), Kernel Stein Discrepancy (KSD), score matching, Wasserstein distances, Sinkhorn divergences, Kullback-Leibler (KL) divergence, and more.</p>
								<div class="table-wrapper">
									<table>
										<tbody>
												<tr>
													<td><strike>Registration and submissions open</strike></td>
													<td>8 May 2023</td>
												</tr>
												<tr>
													<td><strike>Submissions close</strike></td>
													<td>1 June 2023</td>
												</tr>
												<tr>
													<td><strike>Author notifications</strike></td>
													<td>9 June 2023</td>
												</tr>
												<tr>
													<td>Registration closes</td>
													<td>23 June 2023</td>
												</tr>
												<tr>
													<td>Workshop</td>
													<td>27-28 June 2023</td>
												</tr>
									  </tbody>
									</table>
								</div>
							</section>

						<!-- Schedule -->
							<section id="schedule" class="main special">
								<span class="image main"><img src="images/portico2.png" alt="Photo of the main campus of University College London, with the university logo in the top right corner." /></span>
								<header class="major">
									<h2>Programme</h2>
								</header>
								<h2>Tuesday 27 June</h2>
								<div class="table-wrapper">
									<table>
										<tbody>
											<tr>
												<td width=15%>10:00â€“10:30</td>
												<td colspan="2">â˜• Registration and morning coffee</td>
											</tr>
											<tr>
												<td>10:30â€“10:45</td>
												<td>ðŸ‘‹ Welcome from the organisers</td>
											</tr>
											<tr>
												<td>10:45â€“11:30</td>
												<td align="left"><em>Scaling up kernel-based structured output prediction with low-rank approaches.</em>
												<br><b>Florence d'AlchÃ©-Buc</b> (TÃ©lÃ©com Paris, Institut Polytechnique de Paris)
												<br><a class="collapsible">[show abstract]</a>
													<div class="content">
														Surrogate regression methods offer a powerful and flexible solution to structured output prediction by embedding the output in a Hilbert space. In this talk we mainly focus on one the simplest and oldest surrogate approach that leverages kernels in the input space as well as in the output space. While enjoying strong statistical guarantees, these surrogate kernel methods require important computations, for training as well as for inference. We propose to re-visit them by applying low-rank projections in the input and output feature spaces to reduce the complexity. Low-rank projection operators based on sketching are presented and the statistical properties of the resulting novel estimator are studied in terms of excess risk bounds.
														From a computational perspective, we show that the two approximations have distinct but complementary impacts: sketching the input kernel mostly reduces training time, while sketching the output kernel decreases the inference time. In conclusion, we identify other surrogate models based on other losses/distances where this approach could be relevant as well.
													</div>
												</td>
											</tr>
											<tr>
												<td>11:30â€“12:15</td>
												<td align="left"><em>Generalised Bayesian Inference for Intractable Likelihoods.</em>
												<br><b>Takuo Matsubara</b> (The Alan Turing Institute & Newcastle University)
												<br><a class="collapsible">[show abstract]</a>
													<div class="content">
														Generalised Bayesian inference updates prior beliefs using a loss function, rather than a likelihood, and can therefore be used to confer robustness against possible misspecification of the likelihood. Here we consider generalised Bayesian inference with a Stein discrepancy as a loss function, motivated by applications in which the likelihood contains an intractable normalisation constant. In this context, the Stein discrepancy circumvents evaluation of the normalisation constant and produces generalised posteriors that are either closed form or accessible using standard Markov chain Monte Carlo. On a theoretical level, we show consistency, asymptotic normality, and bias-robustness of the generalised posterior, highlighting how these properties are impacted by the choice of Stein discrepancy. Then, we provide numerical experiments on a range of intractable distributions, including applications to kernel-based exponential family models and non-Gaussian graphical models.
													</div>
												</td>
											</tr>
											<tr>
												<td>12:15â€“13:30</td>
												<td>ðŸ¥— Lunch</td>
											</tr>
											<tr>
												<td>13:30-14:15</td>
												<td align="left"><em>Label Shift Quantification via Distribution Feature Matching.</em>
												<br><b>Badr-Eddine ChÃ©rief-Abdellatif</b> (Sorbonne UniversitÃ© & UniversitÃ© Paris CitÃ©)
												<br><a class="collapsible">[show abstract]</a>
													<div class="content">
														Quantification learning deals with the task of estimating the target label distribution under label shift. In this talk, we present a unifying framework, distribution feature matching (DFM), that recovers as particular instances various estimators introduced in previous literature. We derive a general performance bound for DFM procedures and extend this analysis to study robustness of DFM procedures in the misspecified setting under departure from the exact label shift hypothesis, in particular in the case of contamination of the target by an unknown distribution. We also illustrate the theoretical results with a numerical study.
													</div>
												</td>
											</tr>
											<tr>
												<td>14:15-14:35</td>
												<td align="left"><em>TBD.</em>
												<br><b>Masha Naslidnyk</b> (University College London)
												</td>
											</tr>
											<tr>
												<td>14:35-14:55</td>
												<td align="left"><em>TBD.</em>
												<br><b>Oscar Key</b> (University College London)
												</td>
											</tr>
											<tr>
												<td>14:55-15:15</td>
												<td align="left"><em>MMD-FUSE: Learning and Combining Kernels for Two-Sample Testing Without Data Splitting.</em>
												<br><b>Antonin Schrab</b> (University College London)
												<br><a class="collapsible">[show abstract]</a>
													<div class="content">
														We propose novel statistics which maximise the power of a two-sample test based on the Maximum Mean Discrepancy (MMD), by adapting over the set of kernels used in defining it. For finite sets, this reduces to combining (normalised) MMD values under each of these kernels via a weighted soft maximum. Exponential concentration bounds are proved for our proposed statistics under the null and alternative. We further show how these kernels can be chosen in a data-dependent but permutation-independent way, in a well-calibrated test, avoiding data splitting. This technique applies more broadly to general permutation-based MMD testing, and includes the use of deep kernels with features learnt using unsupervised models such as auto-encoders. We highlight the applicability of our MMD-FUSE test on both synthetic low-dimensional and real-world high-dimensional data, and compare its performance in terms of power against current state-of-the-art kernel tests.
													</div>
												</td>
											</tr>
											<tr>
												<td>15:15-15:45</td>
												<td>â˜• Afternoon coffee</td>
											</tr>
											<tr>
												<td>15:45-16:15</td>
												<td>ðŸª§ Poster session</td>
											</tr>
											<tr>
												<td>16:15-17:00</td>
												<td align="left"><em>Variational Gradient Descent using Local Linear Models.</em>
												<br><b>Song Liu</b> (University of Bristol)
												<br><a class="collapsible">[show abstract]</a>
													<div class="content">
														Stein Variational Gradient Descent (SVGD) can transport particles along trajectories that reduce the KL divergence between the target and particle distribution but requires the target score function to compute the update. We introduce a new perspective on SVGD that views it as a local estimator of the reversed KL gradient flow. This perspective inspires us to propose new estimators that use local linear models to achieve the same purpose. The proposed estimators can be computed using only samples from the target and particle distribution without needing the target score function. Our proposed variational gradient estimators utilize local linear models, resulting in computational simplicity while maintaining effectiveness comparable to SVGD in terms of estimation biases. Additionally, we demonstrate that under a mild assumption, the estimation of high-dimensional gradient flow can be translated into a lower-dimensional estimation problem, leading to improved estimation accuracy. We showcase our proposed estimator by transporting non-smiling images in celebA dataset to mimic the distribution of smiling images. The resulting algorithm (dubbed SmileVGD) shows promising performance when transporting images.
													</div>
												</td>
											</tr>
											<tr>
												<td>17:00-17:45</td>
												<td align="left"><em>Convergence control with kernel Stein discrepancies.</em>
												<br><b>Alessandro Barp</b> (University of Cambridge)
												<br><a class="collapsible">[show abstract]</a>
													<div class="content">
														Maximum mean discrepancies (MMDs) like the kernel Stein discrepancy (KSD) have grown central to a wide range of applications, including hypothesis testing, sampler selection, distribution approximation, and variational inference. In each setting, these kernel-based discrepancy measures are required to (i) separate a target P from other probability measures or even (ii) control weak convergence to P. In this talk we discuss the geometry of KSDs and deriveÂ sufficient and necessary conditions to ensure (i) and (ii) hold. In particular we obtain the first KSDs known to exactly metrize weak convergence to P. We highlight the implications of our results for hypothesis testing, measuring and improving sample quality, and sampling with Stein variational gradient descent.
													</div>
												</td>
											</tr>
										</tbody>
									</table>
								</div>
								<br>
								<h2>Wednesday 28 June</h2>
								<div class="table-wrapper">
									<table>
										<tbody>
											<tr>
												<td width=15%>10:00â€“10:30</td>
												<td>â˜• Morning coffee</td>
											</tr>
											<tr>
												<td>10:30â€“11:15</td>
												<td align="left"><em>Using Stein characterisations of network models for goodness-of-fit and data generation.</em>
												<br><b>Gesine Reinert</b> (University of Oxford)
												<br><a class="collapsible">[show abstract]</a>
													<div class="content">
														Synthetic data are increasingly used in statistics and machine learning. A particularly challenging type of data Â are graphs, to Â represent complex dependence structures. Hence methods for generating synthetic data, and for assessing their quality, Â are much in demand. Distributions of random networks can be characterised using Steinâ€™s method. This talk details how these characterisations can be put to use for assessing goodness of fit Â and also how to generate synthetic data.
													</div>
												</td>
											</tr>
											<tr>
												<td>11:15â€“12:00</td>
												<td align="left"><em>Make Steinâ€™s method great again for generative modelling?</em>
												<br><b>Yingzhen Li</b> (Imperial College London)
												<br><a class="collapsible">[show abstract]</a>
													<div class="content">
														My original motivation for studying Steinâ€™s method for ML was to better train deep generative models. Indeed, methods based on Stein discrepancy â€” a clever way to address the intractability issues for score-matching â€” had initial success in training generative models back in a few years ago. But since late 2019, Fisher divergence & demonising methods for score-matching have taken off for large-scale deep generative models, which leads to what we know as score-based generative models (and diffusion models) today in the wave of Generative AI. So in this talk Iâ€™m going to speculate on what needs to be addressed if we want to make Steinâ€™s method great again for training score-based generative models. Itâ€™s possible that Stein discrepancy is less well suited for this purpose, but at least we should try to understand why.
													</div>
												</td>
											</tr>
											<tr>
												<td>12:00â€“13:30</td>
												<td>ðŸ¥— Lunch</td>
											</tr>
											<tr>
												<td>13:30-13:50</td>
												<td align="left"><em>TBD.</em>
												<br><b>MatÃ­as Altamirano</b> (University College London)
												</td>
											</tr>
											<tr>
												<td>13:50-14:10</td>
												<td align="left"><em>Stein Pi-Importance Sampling.</em>
												<br><b>Wilson Chen</b> (University of Sydney)
											<br><a class="collapsible">[show abstract]</a>
												<div class="content">
													Stein discrepancies have emerged as a powerful tool for retrospective improvement of Markov chain Monte Carlo output. However, the question of how to design Markov chains that are well-suited to such post-processing has yet to be addressed. This work studies Stein importance sampling, in which weights are assigned to the states visited by a Pi-invariant Markov chain to obtain a consistent approximation of P, the intended target.
													Surprisingly, the optimal choice of Pi is not identical to the target P; we therefore propose an explicit construction for Pi based on a novel variational argument. Explicit conditions for convergence of Stein Pi-Importance Sampling are established. For 70% of tasks in the PosteriorDB benchmark, a significant improvement over the analogous post-processing of P-invariant Markov chains is reported..
												</div>
												</td>
											</tr>
											<tr>
												<td>14:10-14:30</td>
												<td align="left"><em>Steinâ€™s Method for ErdÃ¶s-RÃ©nyi Mixture Graph Models.</em>
												<br><b>Anum Fatima</b> (University of Oxford)
												<br><a class="collapsible">[show abstract]</a>
													<div class="content">
														In many real world networks the vertices are heterogeneous and the edge probabilities differ between pairs of vertices within the network. The ErdÃ¶s-RÃ©nyi Mixture Graph (ERMG) models accommodate such heterogeneity by assuming that the vertices appear in blocks and the edge probabilities within each block and across the blocks depend on the block membership of the vertices involved. In an ERMG we assume that block membership and the edge probabilities, within and across blocks, are known for all vertices and for all blocks.
														In this work we derive a Stein equation for an ERMG model and obtain a bound on the solution of our Stein equation. Using our Stein equation and the bound, we bound a distance between an ERMG model and an ErdÃ¶s-RÃ©nyi model, an Exponential Random Graph Model (ERGM) and other ErdÃ¶s-RÃ©nyi Mixture Graph models. For an application of our bounds, we use Political blog data from Adamic & Glance (2004) and Florentine Marriage data to give numerical bounds.
														We are further exploring a goodness-of-fit test for the ERMG model using a kernel Stein discrepancy, which is a Monte Carlo test based on the simulated networks. Our test statistics are derived using a divergence constructed using Steinâ€™s method and a discrete Stein operator for the ERMG model taking a reproducing kernel Hilbert space as our Stein class. Xu & Reinert (2021) used this technique to develop a goodness-of-fit test for ERGM.
													</div>
												</td>
											</tr>
											<tr>
												<td>14:30-14:50</td>
												<td align="left"><em>Nonnegative Matrix Factorization in Wasserstein distances for source separation.</em>
												<br><b>Andersen Ang</b> (University of Southampton)
												<br><a class="collapsible">[show abstract]</a>
													<div class="content">
														We consider single-channel (sc) audio blind source separtation (ABSS) via nonnegative matrix factorization (NMF), i.e., given a single measurement of mixed audio recording of multiple musical instruments, we wish to obtain the audio track of each indivudal instrument using NMF. sc-ABSS problem and solving sc-ABSS using NMF is not new, but existing works assume the data is "nice: the data is in a Euclidean space and the NMF method can be used. In this talk we focus on a more practical situation that, due to measurement error, the recording exhibits a misallignment in the frequency spectrum that makes traditional NMF fail. We address this problem using 1-dimensional Wasserstein transformation, and then casting the resulting model as an nonconvex non-smooth non-proximable optimizaiton problem and solved using block coordinate descent with proximal averaging.
													</div>
												</td>
											</tr>
											<tr>
												<td>14:50-15:15</td>
												<td>â˜• Afternoon coffee</td>
											</tr>
											<tr>
												<td>15:15-16:00</td>
												<td align="left"><em>Merging Rates of Opinions via Optimal Transport on Random Measures.</em>
												<br><b>Marta Catalano</b> (University of Warwick)
												<br><a class="collapsible">[show abstract]</a>
													<div class="content">
														The Bayesian approach to inference is based on a coherent probabilistic framework that naturally leads to principled uncertainty quantification and prediction. Via conditional (or posterior) distributions, Bayesian nonparametric models make inference on parameters belonging to infinite-dimensional spaces, such as the space of probability distributions. The development of Bayesian nonparametrics has been triggered by the Dirichlet process, a nonparametric prior that allows one to learn the law of the observations through closed-form expressions. Still, its learning mechanism is often too simplistic and many generalizations have been proposed to increase its flexibility, a popular one being the class of normalized completely random measures. Here we investigate a simple yet fundamental matter: will a different prior actually guarantee a different learning outcome? To this end, we develop a new distance between completely random measures based on optimal transport, which provides an original framework for quantifying the similarity between posterior distributions (or merging of opinions). Our findings provide neat and interpretable insights on the impact of popular Bayesian nonparametric priors, avoiding the usual restrictive assumptions on the data-generating process.
													</div>
												</td>
											</tr>
											<tr>
												<td>16:00-16:45</td>
												<td align="left"><em>Neural signature kernels as infinite-width limits of neural controlled differential equations.</em>
												<br><b>Cristopher Salvi</b> (Imperial College London)
												<br><a class="collapsible">[show abstract]</a>
													<div class="content">
														Motivated by the paradigm of reservoir computing, I will consider randomly initialized neural controlled differential equations and show that in the infinite-width limit and under proper rescaling of the vector fields, these neural architectures converge weakly to Gaussian processes indexed on path-space and with covariances satisfying certain PDEs varying according to the choice of activation function. In the special case where the activation function is the identity, the equation reduces to a linear PDE and the limiting kernel agrees with the original signature kernel.
													</div>
												</td>
											</tr>
										</tbody>
									</table>
								</div>
							</section>

						<!-- Speakers -->
							<section id="speakers" class="main special">
								<header class="major">
									<h2>Invited speakers</h2>
								</header>
								<ul class="features">
									<li>
										<img class="icon major" src="images/alessandro.png" alt="Photo of Alessandro Barp" width="250"/>
										<h3><a href="https://scholar.google.com/citations?user=D5y6_FsAAAAJ&hl=en">Alessandro Barp</a></h3>
										<p>University of Cambridge</p>
									</li>
									<li>
										<img class="icon major" src="images/badr.png" alt="Photo of Badr-Eddine ChÃ©rief-Abdellatif" width="250"/>
										<h3><a href="https://badreddinecheriefabdellatif.github.io/">Badr-Eddine ChÃ©rief-Abdellatif</a></h3>
										<p>Sorbonne UniversitÃ© & UniversitÃ© Paris CitÃ©</p>
									</li>
									<li>
										<img class="icon major" src="images/cris.png" alt="Photo of Cristopher Salvi" width="250"/>
										<h3><a href="https://www.imperial.ac.uk/people/c.salvi">Cristopher Salvi</a></h3>
										<p>Imperial College London</p>
									</li>
									<li>
										<img class="icon major" src="images/florence.jpeg" alt="Photo of Florence d'AlchÃ©-Buc" width="250"/>
										<h3><a href="https://perso.telecom-paristech.fr/fdalche/">Florence d'AlchÃ©-Buc</a></h3>
										<p>TÃ©lÃ©com Paris, Institut Polytechnique de Paris</p>
									</li>
									<li>
										<img class="icon major" src="images/gesine.jpeg" alt="Photo of Gesine Reinert" width="250"/>
										<h3><a href="https://www.stats.ox.ac.uk/~reinert/">Gesine Reinert</a></h3>
										<p>University of Oxford</p>
									</li>
									<li>
										<img class="icon major" src="images/marta.jpeg" alt="Photo of Marta Catalano" width="250"/>
										<h3><a href="https://martacatalano.github.io/">Marta Catalano</a></h3>
										<p>University of Warwick</p>
									</li>
									<li>
										<img class="icon major" src="images/song.jpeg" alt="Photo of Song Liu" width="250"/>
										<h3><a href="https://allmodelsarewrong.net/">Song Liu</a></h3>
										<p>University of Bristol</p>
									</li>
									<li>
										<img class="icon major" src="images/takuo.jpeg" alt="Photo of Takuo Matsubara" width="250"/>
										<h3><a href="https://sites.google.com/view/takuomatsubara/home">Takuo Matsubara</a></h3>
										<p>The Alan Turing Institute & Newcastle University</p>
									</li>
									<li>
										<img class="icon major" src="images/yingzhen.jpeg" alt="Photo of Yingzhen Li" width="250"/>
										<h3><a href="http://yingzhenli.net/">Yingzhen Li</a></h3>
										<p>Imperial College London</p>
									</li>
								</ul>
							</section>

						<!-- Schedule -->
							<section id="organisers" class="main special">
								<header class="major">
									<h2>Organisers</h2>
								</header>
								<ul class="features">
									<li>
										<img class="icon major" src="images/masha.png" alt="Photo of Masha Naslidnyk" width="250"/>
										<h3><a href="https://mashanaslidnyk.github.io/">Masha Naslidnyk</a></h3>
										<p>Co-organiser</p>
									</li>
									<li>
										<img class="icon major" src="images/fx.png" alt="Photo of FranÃ§ois-Xavier Briol" width="250"/>
										<h3><a href="https://fxbriol.github.io/">FranÃ§ois-Xavier Briol</a></h3>
										<p>Co-organiser</p>
									</li>
								</ul>
									<ul class="features">
									<li>
										<img class="icon major" src="images/oscar.png" alt="Photo of Oscar Key" width="250"/>
										<h3><a href="https://oscarkey.github.io/">Oscar Key</a></h3>
										<p>Tech Officer</p>
									</li>
									<li>
										<img class="icon major" src="images/matias.png" alt="Photo of MatÃ­as Altamirano" width="250"/>
										<h3><a href="https://maltamiranomontero.github.io/">MatÃ­as Altamirano</a></h3>
										<p>Logistics Officer</p>
									</li>
									<li>
										<img class="icon major" src="images/ilina.png" alt="Photo of Ilina Yozova" width="250"/>
										<h3><a href="https://uk.linkedin.com/in/ilina-yozova-38b6ba169">Ilina Yozova</a></h3>
										<p>Venue Officer</p>
									</li>
								</ul>
								<p>We are very grateful for funding from the <a href="https://www.grad.ucl.ac.uk/funds/ucl-fellowship-incubator-awards.html">UCL Fellowship Incubator Fund</a>, the <a href="https://www.ucl.ac.uk/statistics/department-statistical-science">UCL Department of Statistical Science</a>, the <a href="https://www.ucl.ac.uk/mathematical-statistical-sciences/institute-mathematical-and-statistical-sciences">UCL Institute for Mathematical and Statistical Sciences (IMSS)</a>, and <a href="https://www.ucl.ac.uk/foundational-ai-cdt/foundational-artificial-intelligence-mphilphd">UKRI CDT in Foundational AI</a> funded by the Engineering and Physical Sciences Research Council [EP/S021566/1].</p>
							</section>
						<!-- Location -->
							<section id="location" class="main special">
								<header class="major">
									<h2>Location</h2>
								</header>
								<p>LG26 Lecture Room, Bentham House, 4-8 Endsleigh Gardens, London, WC1H 0EG.</p>
								<p> <a href="https://www.ucl.ac.uk/laws/about-us/bentham-house">accessibility</a> |
								<a href="https://what3words.com/vote.decent.basin">what3words</a> |
								<a href="https://www.google.co.uk/maps/place/UCL+Bentham+House/">Google Maps </a> </p>
							</section>
							<section>
								<span class="image main"><img src="images/bentham_house.jpeg" alt="Photo of Bentham house, the venue of the workshop." /></span>
							</section>
				</div>

				<!-- Footer -->
					<footer id="footer">
						<p class="copyright">&copy; DBMML workshop. Design by <a href="https://html5up.net">HTML5UP</a>.</p>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
